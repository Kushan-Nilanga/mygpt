{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.46900866, 0.93443552, 0.19270161, 0.55608824],\n",
       "        [0.19776413, 0.28218186, 0.4973711 , 0.34539141],\n",
       "        [0.20529319, 0.72273246, 0.89419725, 0.2827094 ]]),\n",
       " array([[0.22526189, 0.52368148, 0.60756848, 0.61973159, 0.65214266],\n",
       "        [0.03211683, 0.18104348, 0.8501437 , 0.42277902, 0.8360104 ],\n",
       "        [0.56224819, 0.02287917, 0.63988828, 0.16765542, 0.79120056],\n",
       "        [0.89917477, 0.7791288 , 0.27426573, 0.64090183, 0.30197638]]),\n",
       " array([[0.74402754, 0.85245783, 1.3551828 , 1.07442466, 1.40744951],\n",
       "        [0.64382475, 0.43513643, 0.77304134, 0.5466102 , 0.86269773],\n",
       "        [0.82642256, 0.47907977, 1.38887996, 0.76388878, 1.53095322]]))"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "A = np.random.rand(3, 4)\n",
    "B = np.random.rand(4, 5)\n",
    "\n",
    "A, B, np.einsum('ij,jk->ik', A, B)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# free indices: Indeces that are specified in the output\n",
    "# sum indices: Indeces that are not specified in the output\n",
    "\n",
    "# rules\n",
    "# 1. repeated letters in inputs will be multiplied and those products will be the output\n",
    "# 2. ommitted letters in inputs will be summed and those sums will be the output\n",
    "# 3. we can return the unsummed axis in any order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.5410, -0.2934, -2.1788],\n",
      "        [ 0.5684, -1.0845, -1.3986]])\n",
      "tensor([[ 0.3731, -0.1143, -0.4537],\n",
      "        [ 2.3055, -0.3790, -0.4669],\n",
      "        [-0.4609,  1.2652,  1.5746]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 0.3731, -0.3790,  1.5746])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import einsum\n",
    "torch.manual_seed(0)\n",
    "\n",
    "x = torch.randn((2, 3))\n",
    "print(x)\n",
    "\n",
    "# tensor permutation\n",
    "# same as transpose\n",
    "einsum('ij->ji', x)\n",
    "\n",
    "# summation\n",
    "einsum('ij->', x)\n",
    "\n",
    "# column sum\n",
    "einsum('ij->j', x)\n",
    "\n",
    "# row sum\n",
    "einsum('ij->i', x)\n",
    "\n",
    "# matrix vector multiplication\n",
    "v = torch.randn((1, 3))\n",
    "einsum('ij,kj->ik', x, v)\n",
    "\n",
    "# matrix matrix multiplication\n",
    "einsum('ij,kj->ik', x, x)\n",
    "\n",
    "# dot product with matrix\n",
    "einsum('ij,ij->', x, x)\n",
    "\n",
    "# square of matrix\n",
    "einsum('ij,ij->ij', x, x)\n",
    "\n",
    "# outer product\n",
    "a = torch.randn((3))\n",
    "b = torch.randn((5))\n",
    "einsum('i,j->ij', a, b)\n",
    "\n",
    "# batch matrix multiplication\n",
    "a = torch.randn((3, 2, 5))\n",
    "b = torch.randn((3, 5, 3))\n",
    "einsum('ijk,ikl->ijl', a, b)\n",
    "\n",
    "# diagonal\n",
    "a = torch.randn((3, 3))\n",
    "einsum('ii->i', a)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6344bb1c1e8cba1538c6283440edb19bf603374104ac1409d1d721b0d439f293"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
